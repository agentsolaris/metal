{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import metal\n",
    "import os\n",
    "# Import other dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "os.environ['METALHOME'] = '/dfs/scratch1/saelig/slicing/metal/'\n",
    "# Set random seed for notebook\n",
    "SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "Here, the train/test split was defined in the dataset. We then split the train set into a train/valid (see next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io, transform\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "DATASET_DIR = '/dfs/scratch1/saelig/slicing/CUB_200_2011/'\n",
    "IMAGES_DIR = os.path.join(DATASET_DIR, 'images')\n",
    "\n",
    "#Size of eac\n",
    "image_list = np.loadtxt(os.path.join(DATASET_DIR, 'images.txt'), dtype=str)\n",
    "train_test_split = np.loadtxt(os.path.join(DATASET_DIR, 'train_test_split.txt'), dtype=int)\n",
    "labels = np.loadtxt(os.path.join(DATASET_DIR, 'image_class_labels.txt'), dtype=int)\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "#image size (332, 500, 3)\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "tt = transforms.ToTensor()\n",
    "\n",
    "for image_id, image_file in image_list:\n",
    "    image_id = int(image_id)\n",
    "    image_data = io.imread(os.path.join(IMAGES_DIR, image_file))\n",
    "    image_data = transform.resize(image_data, (224,224,3)) #resize all images to 224x224\n",
    "    image_data = normalize(tt(image_data).type(torch.float32)) #make channel dim first\n",
    "    label = labels[image_id - 1][1] #Keep 1 the first class since 0 is used for abstain\n",
    "    if train_test_split[image_id - 1][1] == 1: #put in train\n",
    "        X.append(image_data)\n",
    "        Y.append(label)\n",
    "    else: #put in test\n",
    "        X_test.append(image_data)\n",
    "        Y_test.append(label)\n",
    "\n",
    "X_train = torch.stack(X)\n",
    "Y_train = np.array(Y)\n",
    "X_test = torch.stack(X_test)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's convert all the data to tensors, and create a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metal.utils import split_data\n",
    "\n",
    "(X_train, X_valid), (Y_train, Y_valid) = split_data(X_train, Y_train, splits=[0.8,0.2], seed=SEED)\n",
    "\n",
    "# X_train, X_valid, X_test = torch.tensor(X_train), torch.tensor(X_valid), torch.tensor(X_test)\n",
    "Y_train, Y_valid, Y_test = torch.tensor(Y_train), torch.tensor(Y_valid), torch.tensor(Y_test)\n",
    "\n",
    "# X_train = X_train.permute(0,3,1,2)\n",
    "# X_valid = X_valid.permute(0,3,1,2)\n",
    "# X_test = X_test.permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a task. Use a resnet50 for now as our input model. Since the resnet already includes the fully connected layer, we don't specify a `head_module`, which defaults to the identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchvision.models.resnet import *\n",
    "from metal.mmtl.slicing.tasks import MultiClassificationTask\n",
    "from metal.mmtl.metal_model import MetalModel \n",
    "from resnet import *\n",
    "\n",
    "resnet_model = resnet18(num_classes=200).float().cuda()\n",
    "\n",
    "task0 = MultiClassificationTask(\n",
    "    name='BirdClassificationTask', \n",
    "    input_module=resnet_model, \n",
    ")\n",
    "tasks = [task0]\n",
    "model = MetalModel(tasks, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create payload abstraction for our train/valid/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Payload(Payload0_train: labels_to_tasks=[{'labels': 'BirdClassificationTask'}], split=train),\n",
      " Payload(Payload1_valid: labels_to_tasks=[{'labels': 'BirdClassificationTask'}], split=valid),\n",
      " Payload(Payload2_test: labels_to_tasks=[{'labels': 'BirdClassificationTask'}], split=test)]\n",
      "<metal.mmtl.data.MmtlDataLoader object at 0x7f4c0e575cc0>\n"
     ]
    }
   ],
   "source": [
    "from metal.mmtl.payload import Payload\n",
    "from pprint import pprint\n",
    "\n",
    "payloads = []\n",
    "splits = [\"train\", \"valid\", \"test\"]\n",
    "X_splits = X_train, X_valid, X_test\n",
    "Y_splits = Y_train, Y_valid, Y_test\n",
    "\n",
    "for i in range(3):\n",
    "    payload_name = f\"Payload{i}_{splits[i]}\"\n",
    "    task_name = task0.name\n",
    "    #print(X_splits[i].shape)\n",
    "    payload = Payload.from_tensors(payload_name, {'data': X_splits[i]}, {'labels' : Y_splits[i]}, task_name, splits[i], batch_size=32)\n",
    "    #payload = Payload.from_tensors(payload_name, X_splits[i], Y_splits[i], task_name, splits[i], batch_size=32)\n",
    "    payloads.append(payload)\n",
    "\n",
    "pprint(payloads)\n",
    "print(payloads[0].data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG:  {'verbose': True, 'seed': 531396, 'commit_hash': None, 'ami': None, 'progress_bar': False, 'n_epochs': 30, 'l2': 0.0, 'grad_clip': 1.0, 'optimizer_config': {'optimizer': 'adam', 'optimizer_common': {'lr': 0.001}, 'sgd_config': {'momentum': 0.9}, 'adam_config': {'betas': (0.9, 0.999)}, 'rmsprop_config': {}}, 'lr_scheduler': 'reduce_on_plateau', 'lr_scheduler_config': {'warmup_steps': 0.0, 'warmup_unit': 'epochs', 'min_lr': 1e-06, 'exponential_config': {'gamma': 0.999}, 'plateau_config': {'factor': 0.5, 'patience': 10, 'threshold': 0.0001}}, 'metrics_config': {'task_metrics': [], 'trainer_metrics': ['model/valid/all/loss'], 'aggregate_metric_fns': [], 'max_valid_examples': 0, 'valid_split': 'valid', 'test_split': 'test'}, 'task_scheduler': 'proportional', 'logger': True, 'logger_config': {'log_unit': 'epochs', 'log_every': 2, 'score_every': -1.0, 'log_lr': True}, 'writer': None, 'writer_config': {'log_dir': '/dfs/scratch1/saelig/slicing/metal//logs', 'run_dir': None, 'run_name': None, 'writer_metrics': []}, 'checkpoint': True, 'checkpoint_tasks': False, 'checkpoint_cleanup': True, 'checkpoint_config': {'checkpoint_every': 2, 'checkpoint_best': True, 'checkpoint_metric': 'BirdClassificationTask/Payload1_valid/labels/accuracy', 'checkpoint_metric_mode': 'max', 'checkpoint_dir': None, 'checkpoint_runway': 0}}\n",
      "Beginning train loop.\n",
      "Expecting a total of approximately 4800 examples and 150 batches per epoch from 1 payload(s) in the train split.\n",
      "[2.0 epo]: BirdClassificationTask:[Payload0_train/labels/loss=5.21e+00, Payload1_valid/labels/accuracy=1.92e-02] model:[train/all/loss=5.21e+00, train/all/lr=1.00e-03, valid/all/loss=4.98e+00]\n",
      "Saving model at iteration 2.00 with best (max) score BirdClassificationTask/Payload1_valid/labels/accuracy=0.019\n",
      "[4.0 epo]: BirdClassificationTask:[Payload0_train/labels/loss=4.61e+00, Payload1_valid/labels/accuracy=4.17e-02] model:[train/all/loss=4.61e+00, train/all/lr=1.00e-03, valid/all/loss=4.67e+00]\n",
      "Saving model at iteration 4.00 with best (max) score BirdClassificationTask/Payload1_valid/labels/accuracy=0.042\n",
      "[6.0 epo]: BirdClassificationTask:[Payload0_train/labels/loss=4.17e+00, Payload1_valid/labels/accuracy=5.75e-02] model:[train/all/loss=4.17e+00, train/all/lr=1.00e-03, valid/all/loss=4.35e+00]\n",
      "Saving model at iteration 6.00 with best (max) score BirdClassificationTask/Payload1_valid/labels/accuracy=0.058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object tqdm_notebook.__iter__ at 0x7f4c0e587e08>\n",
      "Traceback (most recent call last):\n",
      "  File \"/dfs/scratch0/saelig/miniconda3/envs/slicing/lib/python3.6/site-packages/tqdm/_tqdm_notebook.py\", line 226, in __iter__\n",
      "    self.sp(bar_style='danger')\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'sp'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-79ecba0e9815>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mcheckpoint_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mcheckpoint_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BirdClassificationTask/Payload1_valid/labels/accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mcheckpoint_metric_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m )\n",
      "\u001b[0;32m/dfs/scratch1/saelig/slicing/metal/metal/mmtl/trainer.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, model, payloads, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;31m#print('batch: ', batch[0]['data'].double())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                 loss_dict, count_dict = model.calculate_loss(\n\u001b[0;32m--> 244\u001b[0;31m                     \u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_to_tasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m                 )\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch1/saelig/slicing/metal/metal/mmtl/metal_model.py\u001b[0m in \u001b[0;36mcalculate_loss\u001b[0;34m(self, X, Ys, payload_name, labels_to_tasks)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mactive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 label_loss = self.loss_hat_funcs[task_name](\n\u001b[0;32m--> 167\u001b[0;31m                     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m                 )\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dfs/scratch1/saelig/slicing/metal/metal/utils.py\u001b[0m in \u001b[0;36mmove_to_device\u001b[0;34m(obj, device)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from metal.mmtl.trainer import MultitaskTrainer\n",
    "trainer = MultitaskTrainer()\n",
    "resnet_model = resnet18(num_classes=200).float().cuda()\n",
    "\n",
    "task0 = MultiClassificationTask(\n",
    "    name='BirdClassificationTask', \n",
    "    input_module=resnet_model, \n",
    ")\n",
    "tasks = [task0]\n",
    "model = MetalModel(tasks, verbose=False)\n",
    "\n",
    "# scores = trainer.train_model(\n",
    "#     model, \n",
    "#     payloads, \n",
    "#     n_epochs=30, \n",
    "#     log_every=2,\n",
    "#     lr=0.001,\n",
    "#     progress_bar=False,\n",
    "# #     lr_scheduler='reduce_on_plateau',\n",
    "# #     patience = 3,\n",
    "#     checkpoint_every = 2,\n",
    "#     checkpoint_metric='BirdClassificationTask/Payload1_valid/labels/accuracy',\n",
    "#     checkpoint_metric_mode='max',\n",
    "# )\n",
    "scores = trainer.train_model(\n",
    "    model, \n",
    "    payloads, \n",
    "    n_epochs=30, \n",
    "    log_every=2,\n",
    "    lr=0.001,\n",
    "    progress_bar=False,\n",
    "    lr_scheduler='reduce_on_plateau',\n",
    "    patience = 10,\n",
    "    checkpoint_every = 2,\n",
    "    checkpoint_metric='BirdClassificationTask/Payload1_valid/labels/accuracy',\n",
    "    checkpoint_metric_mode='max',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see where the model struggles to make accurate predictions by sweeping over the binary attributes. We can then use these to idenitfy potentially useful slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1012)\n",
      "4782\n"
     ]
    }
   ],
   "source": [
    "NUM_ATTRIBUTES = 312\n",
    "attributes_array = np.loadtxt(os.path.join(DATASET_DIR, 'attributes/image_attribute_labels.txt'), usecols=(0,1,2))\n",
    "model = torch.load('resnet18_lr_1e-3_patience10.pt')\n",
    "\n",
    "predictions = torch.tensor(model.predict(payloads[2], task_name='BirdClassificationTask'))\n",
    "print((predictions == (Y_test)).sum())\n",
    "incorrect_predictions = (predictions != Y_test).nonzero().flatten().tolist() #get indices of incorrect predictions\n",
    "print(len(incorrect_predictions))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(attribute id, num misclassifed images)\n",
      "[(146, 3818), (55, 2664), (245, 2653), (219, 2244), (152, 2186), (290, 2180), (21, 2146), (150, 2128), (36, 2004), (260, 1966), (237, 1875), (241, 1828), (236, 1725), (179, 1703), (305, 1688), (164, 1686), (52, 1673), (102, 1654), (210, 1607), (118, 1586), (133, 1533), (70, 1505), (309, 1495), (7, 1479), (312, 1470), (91, 1452), (221, 1429), (194, 1396), (15, 1388), (261, 1382), (254, 1368), (30, 1364), (275, 1361), (132, 1312), (22, 1238), (213, 1237), (269, 1211), (284, 1194), (76, 1191), (117, 1152), (64, 1132), (37, 1123), (195, 1104), (11, 1070), (8, 1058), (244, 1049), (51, 1021), (250, 1016), (240, 984), (188, 981), (299, 971), (58, 956), (26, 952), (158, 928), (214, 884), (78, 881), (311, 875), (180, 862), (173, 860), (209, 851), (111, 833), (45, 827), (85, 822), (60, 790), (222, 780), (126, 770), (248, 751), (71, 749), (165, 747), (306, 734), (101, 731), (203, 712), (2, 684), (263, 683), (184, 679), (295, 677), (24, 666), (92, 655), (154, 626), (39, 610), (54, 610), (239, 610), (293, 594), (120, 590), (169, 585), (5, 562), (212, 558), (75, 541), (226, 537), (73, 534), (197, 533), (81, 531), (46, 510), (79, 508), (255, 500), (204, 495), (278, 495), (107, 494), (105, 480), (228, 480), (243, 469), (112, 451), (135, 449), (308, 449), (41, 445), (103, 430), (216, 430), (249, 415), (167, 412), (182, 412), (77, 410), (57, 408), (6, 401), (151, 378), (199, 378), (285, 378), (247, 369), (94, 365), (215, 365), (294, 360), (122, 359), (310, 357), (25, 346), (217, 343), (100, 336), (291, 336), (10, 335), (153, 334), (230, 331), (183, 330), (289, 320), (96, 318), (280, 310), (229, 305), (127, 302), (238, 299), (59, 287), (265, 284), (31, 266), (16, 261), (233, 260), (119, 256), (80, 252), (231, 252), (56, 251), (168, 249), (53, 244), (246, 243), (218, 237), (121, 235), (262, 233), (274, 227), (134, 225), (242, 223), (189, 221), (211, 218), (97, 217), (259, 217), (147, 215), (292, 214), (98, 211), (65, 210), (256, 206), (99, 200), (104, 200), (159, 198), (307, 198), (50, 192), (106, 192), (166, 186), (148, 184), (4, 183), (174, 182), (232, 182), (40, 180), (300, 180), (223, 179), (116, 176), (235, 174), (208, 170), (276, 168), (95, 164), (234, 161), (279, 158), (32, 154), (74, 154), (196, 151), (198, 151), (277, 146), (17, 141), (137, 140), (35, 138), (20, 137), (66, 134), (27, 133), (190, 131), (23, 130), (38, 125), (257, 124), (33, 121), (9, 118), (296, 118), (251, 117), (301, 117), (87, 115), (185, 115), (273, 114), (155, 113), (224, 113), (12, 111), (18, 110), (86, 110), (47, 108), (123, 108), (160, 106), (163, 106), (141, 105), (61, 100), (175, 99), (253, 99), (193, 98), (44, 96), (72, 96), (205, 96), (42, 95), (304, 95), (131, 94), (140, 93), (113, 92), (220, 92), (1, 91), (67, 91), (108, 89), (69, 87), (191, 85), (202, 83), (3, 82), (93, 81), (110, 81), (90, 80), (181, 79), (200, 79), (264, 79), (178, 78), (283, 76), (48, 75), (139, 75), (302, 75), (128, 73), (270, 73), (82, 72), (149, 71), (170, 71), (157, 69), (206, 69), (114, 66), (125, 65), (145, 65), (298, 65), (187, 63), (14, 61), (161, 59), (29, 58), (136, 58), (225, 58), (129, 57), (252, 55), (268, 53), (88, 51), (176, 51), (156, 44), (186, 44), (281, 43), (297, 43), (288, 39), (63, 38), (192, 37), (28, 36), (172, 36), (258, 36), (124, 35), (130, 35), (303, 33), (49, 32), (115, 32), (13, 31), (207, 31), (19, 30), (43, 30), (84, 30), (286, 30), (89, 28), (68, 27), (266, 27), (267, 26), (109, 25), (227, 25), (34, 24), (62, 24), (201, 24), (177, 23), (83, 22), (144, 22), (162, 22), (171, 19), (142, 15), (287, 15), (271, 14), (282, 13), (138, 11), (143, 7), (272, 6)]\n"
     ]
    }
   ],
   "source": [
    "test_ids = train_test_split[train_test_split[:,1] == 0][:,0]\n",
    "counter = [0] * NUM_ATTRIBUTES\n",
    "for id in incorrect_predictions:\n",
    "    image_id = id + 1\n",
    "    attributes_for_image = attributes_array[attributes_array[:, 0] == image_id][:,1:]\n",
    "    for i in range(len(attributes_for_image)):\n",
    "        if attributes_for_image[i,1] == 1: #attribute is present\n",
    "            counter[i] += 1\n",
    "\n",
    "\n",
    "l = list(map(lambda x: (x[0] + 1, x[1]), enumerate(counter)))\n",
    "print('(attribute id, num misclassifed images)')\n",
    "print(sorted(l, key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
